{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES = './train/images'\n",
    "MASKS = './train/masks'\n",
    "classes = './train/_classes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "       \n",
    "        \n",
    "        \"\"\"\n",
    "        Encoder \n",
    "        \n",
    "        Every block in encoder has 2 convolution layer followed by max pooling layer, except last block which do not have max pooling layer\n",
    "        \n",
    "        The input to the U-Net is 400*400*channels\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.enc_blk11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) \n",
    "        self.enc_blk12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.enc_blk21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.enc_blk22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.enc_blk31 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.enc_blk32 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.enc_blk41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.enc_blk42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.enc_blk51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.enc_blk52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Decoder\n",
    "        Here Upsampling of layers are done\n",
    "        \"\"\"\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec_blk11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.dec_blk12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec_blk21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.dec_blk22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec_blk31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dec_blk32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec_blk41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.dec_blk42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.out_layer = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "       \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Encoder\n",
    "        enc11 = self.relu(self.enc_blk11(x))\n",
    "#         print(\"Encoder block 1 conv layer 1 \", enc11.shape)\n",
    "        enc12 = self.relu(self.enc_blk12(enc11))\n",
    "#         print(\"Encoder block 1 conv layer 2 \",enc12.shape)\n",
    "        pool1 = self.pool(enc12)\n",
    "#         print(\"Encoder block 1 pooling layer \",pool1.shape)\n",
    "        \n",
    "        enc21 = self.relu(self.enc_blk21(pool1))\n",
    "#         print(\"Encoder block 2 conv layer 1 \",enc21.shape)\n",
    "        enc22 = self.relu(self.enc_blk22(enc21))\n",
    "#         print(\"Encoder block 2 conv layer 2 \",enc22.shape)\n",
    "        pool2 = self.pool(enc22)\n",
    "#         print(\"Encoder block 2 pooling layer \",pool2.shape)\n",
    "        \n",
    "        \n",
    "        enc31 = self.relu(self.enc_blk31(pool2))\n",
    "#         print(\"Encoder block 3 conv layer 1 \",enc31.shape)\n",
    "        enc32 = self.relu(self.enc_blk32(enc31))\n",
    "#         print(\"Encoder block 3 conv layer 2 \",enc32.shape)\n",
    "        pool3 = self.pool(enc32)\n",
    "#         print(\"Encoder block 3 pooling layer \",pool3.shape)\n",
    "        \n",
    "        enc41 = self.relu(self.enc_blk41(pool3))\n",
    "#         print(\"Encoder block 4 conv layer 1 \", enc41.shape)\n",
    "        enc42 = self.relu(self.enc_blk42(enc41))\n",
    "#         print(\"Encoder block 4 conv layer 2 \", enc42.shape)\n",
    "        pool4 = self.pool(enc42)\n",
    "#         print(\"Encoder block 4 pooling layer \", pool4.shape)\n",
    "        \n",
    "        enc51 = self.relu(self.enc_blk51(pool4))\n",
    "#         print(\"Encoder block 5 conv layer 1 \", enc51.shape)\n",
    "        enc52 = self.relu(self.enc_blk52(enc51))\n",
    "#         print(\"Encoder block 5 conv layer 2\", enc52.shape)\n",
    "        \n",
    "        # Decoder\n",
    "        \n",
    "        up1 = self.upconv1(enc52)\n",
    "#         print(\"Upsampling layer 1 \", up1.shape)\n",
    "        up11 = torch.cat([up1, enc42], dim=1)\n",
    "#         print(\"Upsampled value layer 1 \", up11.shape)\n",
    "        dec11 = self.relu(self.dec_blk11(up11))\n",
    "#         print(\"Decoder block 1 conv layer 1 \", dec11.shape)\n",
    "        dec12 = self.relu(self.dec_blk12(dec11))\n",
    "        \n",
    "#         print(\"Decoder block 1 conv layer 2\", dec12.shape)\n",
    "        \n",
    "        up2 = self.upconv2(dec12)\n",
    "#         print(\"Upsampling layer 2 \", up2.shape)\n",
    "        up22 = torch.cat([up2, enc32], dim=1)\n",
    "#         print(\"Upsampled value layer 2 \", up22.shape)\n",
    "        dec21 = self.relu(self.dec_blk21(up22))\n",
    "#         print(\"Decoder block 2 conv layer 1 \", dec21.shape)\n",
    "        dec22 = self.relu(self.dec_blk22(dec21))\n",
    "#         print(\"Decoder block 2 conv layer 2 \", dec22.shape)\n",
    "        \n",
    "        up3 = self.upconv3(dec22)\n",
    "#         print(\"Upsampling layer 3 \", up3.shape)\n",
    "        up33 = torch.cat([up3, enc22], dim=1)\n",
    "#         print(\"Upsampled value layer 3 \", up33.shape)\n",
    "        dec31 = self.relu(self.dec_blk31(up33))\n",
    "#         print(\"Decoder block 3 conv layer 1 \", dec31.shape)\n",
    "        dec32 = self.relu(self.dec_blk32(dec31))\n",
    "#         print(\"Decoder block 3 conv layer 2 \", dec32.shape)\n",
    "        \n",
    "        up4 = self.upconv4(dec32)\n",
    "#         print(\"Upsampling layer 4 \", up4.shape)\n",
    "        up44 = torch.cat([up4, enc12], dim=1)\n",
    "#         print(\"Upsampled value layer 4 \", up44.shape)\n",
    "        dec41 = self.relu(self.dec_blk41(up44))\n",
    "#         print(\"Decoder block 4 conv layer 1 \", dec41.shape)\n",
    "        dec42 = self.relu(self.dec_blk42(dec41))\n",
    "#         print(\"Decoder block 4 conv layer 2 \", dec42.shape)\n",
    "        \n",
    "        out = self.out_layer(dec42)\n",
    "#         print('Output ', out.shape)\n",
    "       \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class porosity_Dataset(Dataset):\n",
    "    def __init__(self, image_path, mask_path, x, mean, std, transform=None, patch=False,\n",
    "                 image_ext=\".jpg\", mask_ext=\".png\"):\n",
    "        self.img_path = image_path\n",
    "        self.mask_path = mask_path\n",
    "        self.x = x  # list of base filenames (without extension)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "        self.patch = patch\n",
    "        self.image_ext = image_ext\n",
    "        self.mask_ext = mask_ext\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.x[idx] + self.image_ext\n",
    "        mask_filename = self.x[idx] + \"_mask\" + self.mask_ext\n",
    "\n",
    "        img_path = os.path.join(self.img_path, image_filename)\n",
    "        mask_path = os.path.join(self.mask_path, mask_filename)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img.shape[:2] != mask.shape[:2]:\n",
    "            mask = cv2.resize(mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        # else img is numpy array from cv2 already\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "        t = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(self.mean, self.std)\n",
    "        ])\n",
    "        img = t(img)\n",
    "\n",
    "        mask = torch.tensor(mask, dtype=torch.long)  # convert to tensor explicitly\n",
    "\n",
    "        if self.patch:\n",
    "            img, mask = self.tiles(img, mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def tiles(self, img, mask, threshold=0.01):\n",
    "        size = 256\n",
    "        img_patches = img.unfold(1, size, size).unfold(2, size, size)\n",
    "        img_patches = img_patches.contiguous().view(3, -1, size, size)\n",
    "        img_patches = img_patches.permute(1, 0, 2, 3)\n",
    "\n",
    "        mask_patches = mask.unfold(0, size, size).unfold(1, size, size)\n",
    "        mask_patches = mask_patches.contiguous().view(-1, size, size)\n",
    "\n",
    "        keep_indices = []\n",
    "        for i, patch in enumerate(mask_patches):\n",
    "            fg = (patch != 0).sum().item()\n",
    "            ratio = fg / (size * size)\n",
    "            if ratio > threshold:\n",
    "                keep_indices.append(i)\n",
    "\n",
    "        if not keep_indices:\n",
    "            keep_indices = [0]  # Always return at least one tile\n",
    "\n",
    "        return img_patches[keep_indices], mask_patches[keep_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "t_train = A.Compose([\n",
    "                    A.VerticalFlip(),\n",
    "                    A.HorizontalFlip(),\n",
    "                    A.GridDistortion(p=0.2),\n",
    "                    A.GaussNoise(),\n",
    "                    A.RandomBrightnessContrast((0, 0.5), (0, 0.5))\n",
    "                    ])\n",
    "\n",
    "t_val = A.Compose([\n",
    "                    A.HorizontalFlip(),\n",
    "                    A.GridDistortion(p=0.2)\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = sorted([os.path.splitext(f)[0] for f in os.listdir(IMAGES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train+Val and Test split\n",
    "x_temp, x_test = train_test_split(x_all, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "# Step 2: Train and Val split from the remaining\n",
    "x_train, x_val = train_test_split(x_temp, test_size=0.10, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = porosity_Dataset(\n",
    "    image_path=IMAGES,\n",
    "    mask_path=MASKS,\n",
    "    x=x_train,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    transform=t_train,\n",
    "    patch=True,\n",
    "    image_ext=\".jpg\",   # images are .jpg\n",
    "    mask_ext=\".png\"     # masks are .png\n",
    ")\n",
    "\n",
    "val_dataset = porosity_Dataset(\n",
    "    image_path=IMAGES,\n",
    "    mask_path=MASKS,\n",
    "    x=x_val,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    transform=t_val,\n",
    "    patch=True,\n",
    "    image_ext=\".jpg\",\n",
    "    mask_ext=\".png\"\n",
    ")\n",
    "test_dataset = porosity_Dataset(\n",
    "    image_path=IMAGES,\n",
    "    mask_path=MASKS,\n",
    "    x=x_test,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    transform=t_val,  # You can use the same val transforms for test\n",
    "    patch=True,\n",
    "    image_ext=\".jpg\",\n",
    "    mask_ext=\".png\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # batch is a list of tuples (image_tiles, mask_tiles)\n",
    "    # Here, image_tiles and mask_tiles can be tensors of variable shapes\n",
    "    images = [item[0] for item in batch]\n",
    "    masks = [item[1] for item in batch]\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction_sample(image, mask, output, n_classes=11, class_colors=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    pred = torch.argmax(F.softmax(output.unsqueeze(0), dim=1), dim=1).squeeze(0).cpu()\n",
    "    image = image.permute(1, 2, 0).cpu()\n",
    "    mask = mask.cpu()\n",
    "\n",
    "    if class_colors is None:\n",
    "        class_colors = torch.randint(0, 255, (n_classes, 3), dtype=torch.uint8)\n",
    "\n",
    "    def colorize(mask_tensor):\n",
    "        return class_colors[mask_tensor].numpy().astype('uint8')\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Image')\n",
    "    axes[1].imshow(colorize(mask))\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[2].imshow(colorize(pred))\n",
    "    axes[2].set_title('Prediction')\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = porosity_Dataset(\n",
    "    image_path=IMAGES,\n",
    "    mask_path=MASKS,\n",
    "    x=x_train,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    transform=t_train,\n",
    "    patch=True,\n",
    "    image_ext=\".jpg\",   # images are .jpg\n",
    "    mask_ext=\".png\"     # masks are .png\n",
    ")\n",
    "\n",
    "val_dataset = porosity_Dataset(\n",
    "    image_path=IMAGES,\n",
    "    mask_path=MASKS,\n",
    "    x=x_val,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    transform=t_val,\n",
    "    patch=True,\n",
    "    image_ext=\".jpg\",\n",
    "    mask_ext=\".png\"\n",
    ")\n",
    "test_dataset = porosity_Dataset(\n",
    "    image_path=IMAGES,\n",
    "    mask_path=MASKS,\n",
    "    x=x_test,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    transform=t_val,  # You can use the same val transforms for test\n",
    "    patch=True,\n",
    "    image_ext=\".jpg\",\n",
    "    mask_ext=\".png\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # batch is a list of tuples (image_tiles, mask_tiles)\n",
    "    # Here, image_tiles and mask_tiles can be tensors of variable shapes\n",
    "    images = [item[0] for item in batch]\n",
    "    masks = [item[1] for item in batch]\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=5):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "        \n",
    "        iou_per_class = []\n",
    "        for classes in range(0, n_classes):\n",
    "            true_class = (pred_mask == classes)\n",
    "            true_label = (mask == classes)\n",
    "            \n",
    "            if true_label.long().sum().item() == 0:\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "                \n",
    "                iou = (intersect + smooth) / (union + smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum() / float(correct.numel()))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, test_loader, device, patch=True, visualize=False, num_visuals=5):\n",
    "    model = UNet(11)  \n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    test_iou = 0\n",
    "    test_accuracy = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "            image_tiles, mask_tiles = data\n",
    "\n",
    "            if patch:\n",
    "                images = torch.cat(image_tiles, dim=0).to(device)\n",
    "                masks = torch.cat(mask_tiles, dim=0).to(device)\n",
    "            else:\n",
    "                images = image_tiles.to(device)\n",
    "                masks = mask_tiles.to(device)\n",
    "\n",
    "            if masks.dim() == 4 and masks.size(1) == 11:\n",
    "                masks = torch.argmax(masks, dim=1)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            test_iou += mIoU(outputs, masks)\n",
    "            test_accuracy += pixel_accuracy(outputs, masks)\n",
    "\n",
    "            if visualize and idx < num_visuals:\n",
    "                visualize_prediction_sample(images, outputs, masks)\n",
    "\n",
    "    print(f\"Test mIoU: {test_iou / len(test_loader):.3f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy / len(test_loader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_path = \"./model.pt\"\n",
    "evaluate(model_path, test_loader, device, patch=True, visualize=True, num_visuals=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
