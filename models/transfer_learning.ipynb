{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "from pathlib import Path, PurePath\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Import the new library\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES = './train/images'\n",
    "MASKS = './train/masks'\n",
    "classes_csv = './train/classes.csv'\n",
    "\n",
    "PATCH_IMAGES_DIR = './train_patched/images'\n",
    "PATCH_MASKS_DIR = './train_patched/masks'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tiles(img, mask, size=256, threshold=0.01, min_classes=2):\n",
    "    \"\"\"\n",
    "    Takes a full image and mask and breaks them into filtered patches.\n",
    "    \n",
    "    Args:\n",
    "        img (np.array): The full-size image.\n",
    "        mask (np.array): The full-size mask.\n",
    "        size (int): The height and width of the patches.\n",
    "        threshold (float): The minimum ratio of foreground pixels to keep a patch.\n",
    "        min_classes (int): The minimum number of unique classes to keep a patch.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor): A tuple containing the image patches and mask patches.\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to tensors to use the unfold operation\n",
    "    img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
    "    mask_tensor = torch.from_numpy(mask)\n",
    "\n",
    "    # Create overlapping patches (views) of the tensors\n",
    "    img_patches = img_tensor.unfold(1, size, size).unfold(2, size, size)\n",
    "    img_patches = img_patches.contiguous().view(3, -1, size, size).permute(1, 0, 2, 3)\n",
    "\n",
    "    mask_patches = mask_tensor.unfold(0, size, size).unfold(1, size, size)\n",
    "    mask_patches = mask_patches.contiguous().view(-1, size, size)\n",
    "\n",
    "    # Filter the patches based on content\n",
    "    keep_indices = []\n",
    "    for i, patch in enumerate(mask_patches):\n",
    "        unique_classes = torch.unique(patch)\n",
    "        foreground_pixels = (patch != 0).sum().item()\n",
    "        foreground_ratio = foreground_pixels / (size * size)\n",
    "        \n",
    "        # Keep patch if it has enough classes, enough foreground, or by random chance\n",
    "        if len(unique_classes) >= min_classes or foreground_ratio > threshold or np.random.rand() < 0.1:\n",
    "            keep_indices.append(i)\n",
    "\n",
    "    # Ensure at least one patch is kept to represent the image\n",
    "    if not keep_indices:\n",
    "        keep_indices.append(np.random.randint(0, len(img_patches)))\n",
    "\n",
    "    return img_patches[keep_indices], mask_patches[keep_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched data already exists. Skipping preprocessing.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(PATCH_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(PATCH_MASKS_DIR, exist_ok=True)\n",
    "\n",
    "if not os.listdir(PATCH_IMAGES_DIR):\n",
    "    print(\"Preprocessing patches... This will only run once.\")\n",
    "    original_filenames = sorted([os.path.splitext(f)[0] for f in os.listdir(IMAGES)])\n",
    "    \n",
    "    for filename in tqdm(original_filenames, desc=\"Generating and Saving Patches\"):\n",
    "        img_path = os.path.join(IMAGES, filename + \".jpg\")\n",
    "        mask_path = os.path.join(MASKS, filename + \"_mask.png\")\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if img.shape[:2] != mask.shape[:2]:\n",
    "            mask = cv2.resize(mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Generate filtered patches\n",
    "        img_patches, mask_patches = create_tiles(img, mask)\n",
    "\n",
    "        # Save each patch to the new directories\n",
    "        for i in range(len(img_patches)):\n",
    "            patch_img_np = img_patches[i].permute(1, 2, 0).numpy()\n",
    "            patch_mask_np = mask_patches[i].numpy()\n",
    "\n",
    "            # Save image patch (as PNG to avoid compression artifacts)\n",
    "            save_img_path = os.path.join(PATCH_IMAGES_DIR, f\"{filename}_{i}.png\")\n",
    "            cv2.imwrite(save_img_path, cv2.cvtColor(patch_img_np, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # Save mask patch\n",
    "            save_mask_path = os.path.join(PATCH_MASKS_DIR, f\"{filename}_{i}_mask.png\")\n",
    "            cv2.imwrite(save_mask_path, patch_mask_np)\n",
    "    print(\"Patch preprocessing complete.\")\n",
    "else:\n",
    "    print(\"Patched data already exists. Skipping preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PorosityPatchedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that loads pre-generated and saved image and mask patches.\n",
    "    This is much faster than creating patches on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, mask_dir, filenames, mean, std, transform=None):\n",
    "        self.img_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.x = filenames\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # The filename of the patch (e.g., \"image_name_0.png\")\n",
    "        image_filename = self.x[idx]\n",
    "        # Construct the corresponding mask filename (e.g., \"image_name_0_mask.png\")\n",
    "        mask_filename = os.path.splitext(image_filename)[0] + \"_mask.png\"\n",
    "        \n",
    "        # Construct the full paths\n",
    "        img_path = os.path.join(self.img_dir, image_filename)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_filename)\n",
    "\n",
    "        # Load the image and mask patch from disk\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply augmentations if any\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Apply normalization and convert to tensors\n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        img = t(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "            \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    masks = [item[1] for item in batch]\n",
    "    return images, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, gamma=2, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logpt = -self.ce(input, target)\n",
    "        pt = torch.exp(logpt)\n",
    "        loss = ((1 - pt) ** self.gamma) * -logpt\n",
    "        return loss.mean()\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, n_classes, ignore_index=None):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, target, smooth=1e-6):\n",
    "        input_soft = F.softmax(input, dim=1)\n",
    "        \n",
    "        if self.ignore_index is not None:\n",
    "            mask = target != self.ignore_index\n",
    "            target = target[mask]\n",
    "            input_soft = input_soft.permute(0, 2, 3, 1)[mask.unsqueeze(-1).expand_as(input_soft.permute(0, 2, 3, 1))]\n",
    "            input_soft = input_soft.reshape(-1, self.n_classes)\n",
    "\n",
    "\n",
    "        target_one_hot = F.one_hot(target, num_classes=self.n_classes).float()\n",
    "        \n",
    "        input_flat = input_soft.contiguous().view(-1)\n",
    "        target_flat = target_one_hot.contiguous().view(-1)\n",
    "        \n",
    "        intersection = (input_flat * target_flat).sum()\n",
    "        dice_score = (2. * intersection + smooth) / (input_flat.sum() + target_flat.sum() + smooth)\n",
    "        return 1 - dice_score\n",
    "\n",
    "class DiceFocalLoss(nn.Module):\n",
    "    def __init__(self, n_classes, weight=None, gamma=2, alpha=0.5, ignore_index=-100, ignore_classes=None):\n",
    "        super(DiceFocalLoss, self).__init__()\n",
    "        self.focal_loss = FocalLoss(weight=weight, gamma=gamma, ignore_index=ignore_index)\n",
    "        self.dice_loss = DiceLoss(n_classes=n_classes, ignore_index=ignore_index)\n",
    "        self.alpha = alpha\n",
    "        self.ignore_index = ignore_index\n",
    "        self.ignore_classes = ignore_classes if ignore_classes is not None else []\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target_for_loss = target.clone()\n",
    "\n",
    "        for cls_idx in self.ignore_classes:\n",
    "            target_for_loss[target_for_loss == cls_idx] = self.ignore_index\n",
    "        \n",
    "        focal = self.focal_loss(input, target_for_loss)\n",
    "        dice = self.dice_loss(input, target_for_loss)\n",
    "        \n",
    "        return self.alpha * focal + (1 - self.alpha) * dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=6):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "        \n",
    "        iou_per_class = []\n",
    "        for c in range(n_classes):\n",
    "            true_class = pred_mask == c\n",
    "            true_label = mask == c\n",
    "            \n",
    "            if true_label.long().sum().item() == 0:\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "                iou = (intersect + smooth) / (union + smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_class_iou(preds, labels, num_classes=6):\n",
    "    ious = []\n",
    "    preds = preds.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = preds == cls\n",
    "        label_inds = labels == cls\n",
    "        intersection = (pred_inds & label_inds).sum().item()\n",
    "        union = (pred_inds | label_inds).sum().item()\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "    return ious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, num_classes=6):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_iou, val_iou = [], []\n",
    "    train_acc, val_acc = [], []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    no_improve = 0\n",
    "\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        running_loss, running_iou, running_acc, total_batches = 0, 0, 0, 0\n",
    "\n",
    "        # The loop is now simpler as the dataloader yields batched tensors directly\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_iou += mIoU(outputs, masks)\n",
    "            running_acc += pixel_accuracy(outputs, masks)\n",
    "            total_batches += 1\n",
    "            lrs.append(get_lr(optimizer))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_iou_score, val_accuracy, val_batches = 0, 0, 0, 0\n",
    "        all_class_ious = []\n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                val_loss += criterion(outputs, masks).item()\n",
    "                val_iou_score += mIoU(outputs, masks)\n",
    "                val_accuracy += pixel_accuracy(outputs, masks)\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                for pred, true_mask in zip(preds, masks):\n",
    "                    all_class_ious.append(compute_per_class_iou(pred, true_mask, num_classes))\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_train_loss = running_loss / total_batches\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        avg_train_iou = running_iou / total_batches\n",
    "        avg_val_iou = val_iou_score / val_batches\n",
    "        avg_train_acc = running_acc / total_batches\n",
    "        avg_val_acc = val_accuracy / val_batches\n",
    "\n",
    "        train_losses.append(avg_train_loss); val_losses.append(avg_val_loss)\n",
    "        train_iou.append(avg_train_iou); val_iou.append(avg_val_iou)\n",
    "        train_acc.append(avg_train_acc); val_acc.append(avg_val_acc)\n",
    "        \n",
    "        mean_class_ious = np.nanmean(np.array(all_class_ious), axis=0)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Train mIoU: {avg_train_iou:.4f}, Val mIoU: {avg_val_iou:.4f}, \"\n",
    "              f\"Train Acc: {avg_train_acc:.4f}, Val Acc: {avg_val_acc:.4f}, \"\n",
    "              f\"LR: {lrs[-1]:.6f}, \"\n",
    "              f\"Time: {(time.time() - epoch_start)/60:.2f} mins\")\n",
    "\n",
    "        print(\"\\nðŸ“Š Per-class IoU (Validation):\")\n",
    "        for i, iou in enumerate(mean_class_ious):\n",
    "            print(f\"  Class {i}: IoU = {iou:.4f}\")\n",
    "\n",
    "        if avg_val_loss < min_loss:\n",
    "            print(f\"Validation loss decreased ({min_loss:.4f} -> {avg_val_loss:.4f}). Saving model.\")\n",
    "            min_loss = avg_val_loss\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), 'model_transfer_learning.pt')\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            print(f\"No improvement in validation loss for {no_improve} epochs.\")\n",
    "            if no_improve >= 10:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    total_time = (time.time() - start_time) / 60\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} minutes.\")\n",
    "    history = {'train_loss': train_losses, 'val_loss': val_losses, 'train_miou': train_iou, 'val_miou': val_iou, 'train_acc': train_acc, 'val_acc': val_acc, 'lrs': lrs}\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = A.Compose([A.VerticalFlip(), A.HorizontalFlip(), A.GridDistortion(p=0.2),\n",
    "                     A.GaussNoise(), A.RandomBrightnessContrast((0, 0.5), (0, 0.5))])\n",
    "t_val = A.Compose([A.HorizontalFlip(), A.GridDistortion(p=0.2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_patches = sorted(os.listdir(PATCH_IMAGES_DIR))\n",
    "x_temp, x_test = train_test_split(x_all_patches, test_size=0.10, random_state=42)\n",
    "x_train, x_val = train_test_split(x_temp, test_size=0.10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PorosityPatchedDataset(PATCH_IMAGES_DIR, PATCH_MASKS_DIR, x_train, mean, std, t_train)\n",
    "val_dataset = PorosityPatchedDataset(PATCH_IMAGES_DIR, PATCH_MASKS_DIR, x_val, mean, std, t_val)\n",
    "test_dataset = PorosityPatchedDataset(PATCH_IMAGES_DIR, PATCH_MASKS_DIR, x_test, mean, std, t_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution: [149582430    401226    410354  30316280     73468    161138]\n",
      "Val class distribution: [16804742    71563     5712  3207562    11054    18919]\n"
     ]
    }
   ],
   "source": [
    "def get_class_distribution(dataset, num_classes=6):\n",
    "    class_counts = np.zeros(num_classes, dtype=int)\n",
    "    for _, mask in DataLoader(dataset, batch_size=1):\n",
    "        for m in mask:\n",
    "            for cls in range(num_classes):\n",
    "                class_counts[cls] += torch.sum(m == cls).item()\n",
    "    return class_counts\n",
    "\n",
    "train_class_counts = get_class_distribution(train_dataset)\n",
    "val_class_counts = get_class_distribution(val_dataset)\n",
    "\n",
    "print(\"Train class distribution:\", train_class_counts)\n",
    "print(\"Val class distribution:\", val_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights_from_masks(\n",
    "    masks_dir, \n",
    "    num_classes=6, \n",
    "    background_class_id=0, \n",
    "    background_weight_multiplier=0.1\n",
    "):\n",
    "    class_counts = np.zeros(num_classes, dtype=np.int64)\n",
    "    total_pixels = 0\n",
    "\n",
    "    # Calculate weights from the new patched masks for a more accurate distribution\n",
    "    for mask_name in os.listdir(masks_dir):\n",
    "        if mask_name.endswith(\".png\"):\n",
    "            mask_path = os.path.join(masks_dir, mask_name)\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            if mask is not None:\n",
    "                total_pixels += mask.size\n",
    "                for cls in range(num_classes):\n",
    "                    class_counts[cls] += np.sum(mask == cls)\n",
    "    class_frequencies = class_counts / total_pixels\n",
    "    class_frequencies[class_frequencies == 0] = 1e-6\n",
    "    class_weights = 1.0 / class_frequencies\n",
    "    class_weights[background_class_id] *= background_weight_multiplier\n",
    "    class_weights = class_weights * (num_classes / np.sum(class_weights))\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: tensor([1.6789e-04, 6.4128e-01, 6.3926e-01, 8.6391e-03, 3.1442e+00, 1.5664e+00],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "class_weights = get_class_weights_from_masks(PATCH_MASKS_DIR, num_classes=6)\n",
    "class_weights = class_weights.to(device)\n",
    "print(f\"Using class weights: {class_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 1e-3\n",
    "epoch = 30\n",
    "weight_decay = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DiceFocalLoss(\n",
    "    n_classes=6,\n",
    "    weight=class_weights,\n",
    "    gamma=2,\n",
    "    alpha=0.5,\n",
    "    ignore_index=0,\n",
    "    ignore_classes=[2]\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch, steps_per_epoch=len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:  21%|â–ˆâ–ˆ        | 36/173 [00:43<02:43,  1.19s/it]"
     ]
    }
   ],
   "source": [
    "history = fit(epoch, model, train_loader, val_loader, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history['val_loss'], label='val', marker='o')\n",
    "    plt.plot(history['train_loss'], label='train', marker='o')\n",
    "    plt.title('Loss per epoch'); plt.ylabel('loss');\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_score(history):\n",
    "    plt.plot(history['train_miou'], label='train_mIoU', marker='*')\n",
    "    plt.plot(history['val_miou'], label='val_mIoU',  marker='*')\n",
    "    plt.title('Score per epoch'); plt.ylabel('mean IoU')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_acc(history):\n",
    "    plt.plot(history['train_acc'], label='train_accuracy', marker='*')\n",
    "    plt.plot(history['val_acc'], label='val_accuracy',  marker='*')\n",
    "    plt.title('Accuracy per epoch'); plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(history)\n",
    "plot_score(history)\n",
    "plot_acc(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
